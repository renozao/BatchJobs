## Job Resource Interface Definition
##
## R [R-2.14.1-icc-11.1,      R version (and compiler) we want to use.
##    R-2.15.1-gcc-4.3.5,     We have different R versions on LiDO installed, some compiled with GCC
##    R-2.15.1-gcc-4.7.1,     and others with Intel compilers. Some packages fail to build
##    R-2.15.3-gcc-4.7.2,     with ICC, but R compiled with ICC is in general faster.
##    R-2.15.3-gcc-4.8.0,     From R-2.15.1-gcc-4.7.1 on all R versions should have gotoblas support and should in general be
##    R-3.0.1-gcc-4.8.1-base, faster for linear algebra operations, otherwise complain!
##    R-3.1.0-gcc-4.8.2-base,
##    R-3.2.2-gcc-4.8.5-base]
##
## modules [character]:       Extra modules you would like to load on slaves. Please note that the following
##                            modules are handled automatically by the previous argument "R" and should not be
##                            passed manually: R, icc, gcc, binutils, gotoblas, openmpi, torque, maui.
## nodes [integer(1)]:        Number of required nodes,
##                            Set larger than 1 if you want to further parallelize with MPI within your job
## parcpus [integer(1)]:      Alternative name for 'nodes'.
##                            Offered so we have consistent names for Torque and SLURM.
## walltime [integer(1)]:     Walltime for this job, in seconds.
##                            Must be at least 60 seconds.
## memory   [integer(1)]:     Memory in megabytes for each job. Is mapped to vmem on Lido.
##                            Must be at least 100 (when I tried lower values my jobs did not start at all).
##                            For jobs with no internal parallelization
##                            (i.e., nodes = 1) directly mapped to vmem on Lido.
##                            In this case it is simply the amount of memory required for each R job.
##                            If you use multiple nodes with MPI, vmem = 4 * memory,
##                            as at most 4 jobs can potentially be scheduled to the cores of the same node.
##                            Here, you set the memory of each MPI job.
##
## 'walltime' and 'memory' settings automatically determine the correct queue, you don't have to
## select the queue yourself.
## Default resources can be set in your .BatchJobs.R by defining the variable
## 'default.resources' as a named list.

<%
d = setdiff(names(resources), c("walltime", "memory", "nodes", "parcpus", "R", "modules"))
if (length(d) > 0)
  stopf("Illegal resources used: %s", collapse(d))

nodes = resources$nodes
parcpus = resources$parcpus
R = resources$R
modules = resources$modules
walltime = asInt(resources$walltime, lower = 60L, upper = 60L * 60L * 24L * 28L)
memory = asInt(resources$memory, lower = 100L, upper = 64L * 1024L)

if (!is.null(nodes))
  nodes = asInt(nodes, lower = 1L)
if (!is.null(parcpus))
  parcpus = asInt(parcpus, lower = 1)
if (is.null(nodes) && is.null(parcpus))
  stopf("You must set either 'nodes' or 'parcpus'!")
if (!is.null(nodes) && !is.null(parcpus))
  stopf("You cannot set both 'nodes' and 'parcpus'!")
if (is.null(nodes))
  nodes = parcpus
assertChoice(R, choices = c(
  "R-2.14.1-icc-11.1", "R-2.15.1-gcc-4.3.5",
  "R-2.15.1-gcc-4.7.1", "R-2.15.3-gcc-4.7.2", "R-2.15.3-gcc-4.8.0",
  "R-3.0.1-gcc-4.8.1-base",
  "R-3.1.0-gcc-4.8.2-base",
  "R-3.2.2-gcc-4.8.5-base"
))
assertCharacter(modules, any.missing = FALSE)

cmd = "R CMD BATCH --no-save --no-restore"

# first string of queue, selected by walltime
s1 = if (walltime <= 3600) {
  "short"
} else if (walltime <= 8 * 3600) {
  "med"
} else if (walltime <= 2 * 24 * 3600) {
  "long"
} else if (walltime <= 28 * 24 * 3600) {
  "ultralong"
}

queue.max.mem = c(eth = 15000, quad = 61440)

if (nodes == 1L)  {
  vmem = memory
} else {
  # for MPI jobs at most 4 jobs get scheduled on the cores of one node
  vmem = 4 * memory
  cmd = paste("mpirun -np 1", cmd)
}
if (vmem <= queue.max.mem["eth"]) {
  s2 = "eth"
} else if (vmem <= queue.max.mem["quad"]) {
  s2 = "quad"
} else {
  stop("You are requesting too much memory, there is no queue for this!")
}
## now put two parts s1/2 for queue selection together
queue = paste(s1, s2, sep="_")
if (queue == "ultralon_quad")
  stop("No 'ultralong_quad' queue available. Reduce memory or walltime.")

## get the right software modules to load depending on
## parallelization (nodes > 1) and compiler
modules2 = switch(R,
  "R-2.14.1-icc-11.1" =  c("openmpi/ge/intel11.1/64/1.4.2", "intel/cce/11.1.075", "intel/fce/11.1.075", "R/2.14.1-icc"),
  "R-2.15.1-gcc-4.3.5" = c("openmpi/ge/gcc4.3.5/64/1.4.2", "gcc/4.3.5", "R/2.15.1-gcc"),
  "R-2.15.1-gcc-4.7.1" = c("openmpi/ge/gcc4.7.x/64/1.4.5", "binutils", "gcc/4.7.1", "gotoblas/shared/64/1.26", "R/2.15.1-gcc47"),
  "R-2.15.3-gcc-4.7.2" = c("openmpi/ge/gcc4.7.x/64/1.4.5", "binutils", "gcc/4.7.2", "gotoblas/shared/64/1.26", "R/2.15.3-gcc47"),
  "R-2.15.3-gcc-4.8.0" = c("openmpi/ge/gcc4.8.x/64/1.6.4", "binutils", "gcc/4.8.0", "gotoblas/shared/64/1.26", "R/2.15.3-gcc48"),
  "R-3.0.1-gcc-4.8.1-base" = c("openmpi/ge/gcc4.8.x/64/1.6.4", "binutils", "gcc/4.8.1", "gotoblas/shared/64/1.26", "R/3.0.1-gcc48-base"),
  "R-3.2.2-gcc-4.8.5-base" = c("openmpi/ge/gcc4.8.x/64/1.6.4", "binutils", "gcc/4.8.5", "gotoblas/shared/64/1.26", "R/3.2.2-gcc48-base")
)
if(nodes == 1L)
  modules2 = modules2[-1]
modules2 = union(modules2, modules)
modules2 = collapse(modules2, sep=" ")

## very ugly hack because we cannot log to data (nobackup) filesystem on lido,
## only home fs is available
## unfortunately there seems to be no generic solution
## does log path start with /data/?
if (length(grep("^/data/", log.file)) > 0) {
  ## strip that
  log.file2 = substr(log.file, 7, nchar(log.file))
  ## find next forward slash
  i = regexpr("/", log.file2)
  if (i != -1) {
    ## this must be "user": e.g. /data/bischl/...
    user = substr(log.file2, 1, i-1)
    ## put together
    log.file = sprintf("/home/%s/nobackup%s", user, substr(log.file2, i, nchar(log.file2)))
  }
}

-%>

#PBS -N <%= job.name %>
#PBS -j oe
#PBS -o <%= log.file %>

#PBS -l walltime=<%= walltime %>,nodes=<%= nodes %>,vmem=<%= vmem %>M
#PBS -q <%= queue %>

## setup modules
source /sysdata/shared/sfw/Modules/default/init/bash
module add <%= modules2 %>

## create our own temp dir and clean it up later, lido does not do this automatically
mkdir /scratch/${USER}-${PBS_JOBID}
export TMPDIR=/scratch/${USER}-${PBS_JOBID}
## run R
<%= cmd %> "<%= rscript %>" /dev/stdout
rm -fR /scratch/${USER}-${PBS_JOBID}
